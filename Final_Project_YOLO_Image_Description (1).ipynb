{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision opencv-python pytesseract transformers\n",
        "!pip install tesseract\n",
        "!pip install pytesseract\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaTZbYqYwVuC",
        "outputId": "5ef2a829-d854-463d-9bac-d7134a4fb17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (11.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/argman/EAST.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # clone and install\n",
        "  !git clone -q $git_repo_url\n",
        "  #!cd $project_name && pip install -q -r requirements.txt\n",
        "  # patch for the Python 3.7\n",
        "  !sed -i 's/python3-config/python3.7-config/g' EAST/lanms/Makefile\n",
        "\n",
        "import sys\n",
        "sys.path.append(project_name)\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "plt.rcParams[\"axes.grid\"] = False"
      ],
      "metadata": {
        "id": "SIn_Mk8_oxFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=$file_id\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  #confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=$confirm_text&id=$file_id\" -o $file_name\n",
        "\n",
        "\n",
        "pretrained_model = 'east_icdar2015_resnet_v1_50_rbox'\n",
        "if not exists(pretrained_model):\n",
        "  # download the pretrained model\n",
        "  pretrained_model_file_name = 'east_icdar2015_resnet_v1_50_rbox.zip'\n",
        "  download_from_google_drive('0B3APw5BZJ67ETHNPaU9xUkVoV0U', pretrained_model_file_name)\n",
        "  !unzip $pretrained_model_file_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuvFhn0Qox78",
        "outputId": "f7939a29-0277-4c8b-f780-786ef0c0a745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  346M  100  346M    0     0  37.4M      0  0:00:09  0:00:09 --:--:-- 23.4M\n",
            "Archive:  east_icdar2015_resnet_v1_50_rbox.zip\n",
            "  inflating: east_icdar2015_resnet_v1_50_rbox/checkpoint  \n",
            "  inflating: east_icdar2015_resnet_v1_50_rbox/model.ckpt-49491.data-00000-of-00001  \n",
            "  inflating: east_icdar2015_resnet_v1_50_rbox/model.ckpt-49491.index  \n",
            "  inflating: east_icdar2015_resnet_v1_50_rbox/model.ckpt-49491.meta  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wxL7RJLevcfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4FWpORUQC1Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import pytesseract\n",
        "from imutils.object_detection import non_max_suppression\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "def decode_predictions(scores, geometry, min_confidence):\n",
        "    (numRows, numCols) = scores.shape[2:4]\n",
        "    rects = []\n",
        "    confidences = []\n",
        "    for y in range(0, numRows):\n",
        "        scoresData = scores[0, 0, y]\n",
        "        xData0 = geometry[0, 0, y]\n",
        "        xData1 = geometry[0, 1, y]\n",
        "        xData2 = geometry[0, 2, y]\n",
        "        xData3 = geometry[0, 3, y]\n",
        "        anglesData = geometry[0, 4, y]\n",
        "\n",
        "        # Loop over the number of columns\n",
        "        for x in range(0, numCols):\n",
        "            if scoresData[x] < min_confidence:\n",
        "                continue\n",
        "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
        "            angle = anglesData[x]\n",
        "            cos = np.cos(angle)\n",
        "            sin = np.sin(angle)\n",
        "            h = xData0[x] + xData2[x]\n",
        "            w = xData1[x] + xData3[x]\n",
        "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
        "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
        "            startX = int(endX - w)\n",
        "            startY = int(endY - h)\n",
        "            rects.append((startX, startY, endX, endY))\n",
        "            confidences.append(scoresData[x])\n",
        "    return (rects, confidences)\n",
        "\n",
        "# (rects, confidences) = decode_predictions(scores, geometry, min_confidence)\n",
        "# boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
        "\n",
        "def detect_text(image, east_model_path, min_confidence=0.5, width=320, height=320):\n",
        "    # Check if image is a string (path), then load it\n",
        "    if isinstance(image, str):\n",
        "        image = cv2.imread(image)\n",
        "        if image is None:\n",
        "            raise ValueError(\"Image not found at provided path.\")\n",
        "    orig = image.copy()\n",
        "    (origH, origW) = image.shape[:2]\n",
        "    rW = origW / float(width)\n",
        "    rH = origH / float(height)\n",
        "    image = cv2.resize(image, (width, height))\n",
        "    (H, W) = image.shape[:2]\n",
        "\n",
        "    # Define the two output layer names for the EAST detector model\n",
        "    layerNames = [\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"]\n",
        "    net = cv2.dnn.readNet(east_model_path)\n",
        "    # Construct a blob from the image and then perform a forward pass of the model\n",
        "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    (scores, geometry) = net.forward(layerNames)\n",
        "    (rects, confidences) = decode_predictions(scores, geometry, min_confidence)\n",
        "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
        "    results = []\n",
        "    for (startX, startY, endX, endY) in boxes:\n",
        "        startX = int(startX * rW)\n",
        "        startY = int(startY * rH)\n",
        "        endX = int(endX * rW)\n",
        "        endY = int(endY * rH)\n",
        "        results.append((startX, startY, endX, endY))\n",
        "\n",
        "    return results\n",
        "\n",
        "def recognize_text(image, bounding_boxes):\n",
        "    recognized_texts = []\n",
        "    for box in bounding_boxes:\n",
        "        x, y, w, h = box\n",
        "        roi = image[y:y+h, x:x+w]\n",
        "        text = pytesseract.image_to_string(roi)\n",
        "        recognized_texts.append(text)\n",
        "    return recognized_texts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDescriptionPipeline:\n",
        "    def __init__(self, east_model_path):\n",
        "        # Load YOLOv8 (using YOLOv5 as a placeholder)\n",
        "        self.yolov8_model = YOLO('yolov8n.pt')\n",
        "        # Load GPT-2\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        self.gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        self.east_model_path = east_model_path\n",
        "\n",
        "    def detect_objects(self, image):\n",
        "        class_name = self.yolov8_model.names[int(class_id)]\n",
        "\n",
        "        results = self.yolov8_model(image)\n",
        "        detected_objects = []\n",
        "        if hasattr(results, 'xyxy'):\n",
        "            # Iterate over detections\n",
        "            for detection in results.xyxy[0]:\n",
        "                class_id = int(detection[5])\n",
        "                confidence = float(detection[4])\n",
        "                x_min, y_min, x_max, y_max = map(int, detection[:4])\n",
        "                detected_objects.append({\n",
        "                    'class_id': class_id,\n",
        "                    'confidence': confidence,\n",
        "                    'bbox': [x_min, y_min, x_max, y_max]\n",
        "                })\n",
        "        else:\n",
        "            # Handle cases where the output is not as expected\n",
        "            print(\"Unexpected output format from YOLO model\")\n",
        "        return detected_objects\n",
        "\n",
        "\n",
        "    def detect_and_recognize_text(self, image_path):\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(\"Image not found at provided path.\")\n",
        "        bounding_boxes = detect_text(image, self.east_model_path)\n",
        "        recognized_texts = recognize_text(image, bounding_boxes)\n",
        "        return recognized_texts\n",
        "\n",
        "    def generate_description(self, objects, texts):\n",
        "        # Generate description with GPT-2\n",
        "        input_text = \"Detected objects: \" + \", \".join([str(obj['class_id']) for obj in objects])\n",
        "        input_text += \". Detected texts: \" + \", \".join(texts)\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
        "        # Generate description using GPT-2\n",
        "        outputs = self.gpt2_model.generate(inputs['input_ids'], max_length=525, num_return_sequences=1)\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def describe_image(self, image_path):\n",
        "        # Describe an image\n",
        "        # image = cv2.imread(image_path)\n",
        "        objects = self.detect_objects(image)\n",
        "        texts = self.detect_and_recognize_text(image)\n",
        "        description = self.generate_description(objects, texts)\n",
        "        return description\n",
        "\n",
        "# Example usage\n",
        "east_model_path = '/content/frozen_east_text_detection.pb'\n",
        "pipeline = ImageDescriptionPipeline(east_model_path)\n",
        "image = '124.jpg'\n",
        "description = pipeline.describe_image(image)\n",
        "print(description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zvTmr-MsyPMx",
        "outputId": "f0c32b46-f43e-4dbb-cc9c-8499e4df5184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-ea1f594c24b8>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDescriptionPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meast_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'124.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-ea1f594c24b8>\u001b[0m in \u001b[0;36mdescribe_image\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Describe an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# image = cv2.imread(image_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_and_recognize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-ea1f594c24b8>\u001b[0m in \u001b[0;36mdetect_objects\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdetect_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolov8_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolov8_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'class_id' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Try"
      ],
      "metadata": {
        "id": "CB6pYYjF0tZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to decode predictions from EAST detector\n",
        "def decode_predictions(scores, geometry, min_confidence):\n",
        "    (numRows, numCols) = scores.shape[2:4]\n",
        "    rects = []\n",
        "    confidences = []\n",
        "    for y in range(numRows):\n",
        "        scoresData = scores[0, 0, y]\n",
        "        xData0 = geometry[0, 0, y]\n",
        "        xData1 = geometry[0, 1, y]\n",
        "        xData2 = geometry[0, 2, y]\n",
        "        xData3 = geometry[0, 3, y]\n",
        "        anglesData = geometry[0, 4, y]\n",
        "        for x in range(numCols):\n",
        "            if scoresData[x] < min_confidence:\n",
        "                continue\n",
        "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
        "            angle = anglesData[x]\n",
        "            cos = np.cos(angle)\n",
        "            sin = np.sin(angle)\n",
        "            h = xData0[x] + xData2[x]\n",
        "            w = xData1[x] + xData3[x]\n",
        "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
        "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
        "            startX = int(endX - w)\n",
        "            startY = int(endY - h)\n",
        "            rects.append((startX, startY, endX, endY))\n",
        "            confidences.append(scoresData[x])\n",
        "    return (rects, confidences)\n",
        "\n",
        "# Function to detect text in an image\n",
        "def detect_text(image, east_model_path, min_confidence=0.5, width=320, height=320):\n",
        "    if isinstance(image, str):\n",
        "        image = cv2.imread(image)\n",
        "        if image is None:\n",
        "            raise ValueError(\"Image not found at provided path.\")\n",
        "    orig = image.copy()\n",
        "    (origH, origW) = image.shape[:2]\n",
        "    rW = origW / float(width)\n",
        "    rH = origH / float(height)\n",
        "    image = cv2.resize(image, (width, height))\n",
        "    (H, W) = image.shape[:2]\n",
        "    layerNames = [\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"]\n",
        "    net = cv2.dnn.readNet(east_model_path)\n",
        "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    (scores, geometry) = net.forward(layerNames)\n",
        "    (rects, confidences) = decode_predictions(scores, geometry, min_confidence)\n",
        "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
        "    results = []\n",
        "    for (startX, startY, endX, endY) in boxes:\n",
        "        startX = int(startX * rW)\n",
        "        startY = int(startY * rH)\n",
        "        endX = int(endX * rW)\n",
        "        endY = int(endY * rH)\n",
        "        results.append((startX, startY, endX, endY))\n",
        "    return results\n",
        "\n",
        "def recognize_text(image, bounding_boxes):\n",
        "    recognized_texts = set()  # Using a set to automatically avoid duplicates\n",
        "    for box in bounding_boxes:\n",
        "        x, y, w, h = box\n",
        "        roi = image[y:y+h, x:x+w]\n",
        "        text = pytesseract.image_to_string(roi).strip()\n",
        "        if text:  # Only add non-empty text\n",
        "            recognized_texts.add(text)\n",
        "    return list(recognized_texts)"
      ],
      "metadata": {
        "id": "59UHVNXcQ7u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class ImageDescriptionPipeline:\n",
        "    def __init__(self, east_model_path, yolo_model_path):\n",
        "        # Load YOLOv8\n",
        "        self.yolov8_model = YOLO(yolo_model_path)\n",
        "        # Load GPT-2\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        self.gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        self.east_model_path = east_model_path\n",
        "\n",
        "    def detect_objects(self, image_path):\n",
        "        # Load the image using OpenCV\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(\"Image not found at provided path: \" + image_path)\n",
        "\n",
        "        # Perform object detection using YOLO\n",
        "        results = self.yolov8_model(image)\n",
        "\n",
        "        # Process detection results\n",
        "        detected_objects = []\n",
        "        for result in results.pred:\n",
        "            for det in result:\n",
        "                x_min, y_min, x_max, y_max, confidence, class_id = det.tolist()\n",
        "                class_name = self.yolov8_model.names[int(class_id)] if int(class_id) < len(self.yolov8_model.names) else \"Unknown\"\n",
        "\n",
        "                detected_objects.append({\n",
        "                    'class_id': int(class_id),\n",
        "                    'class_name': class_name,\n",
        "                    'confidence': float(confidence),\n",
        "                    'bbox': [int(x_min), int(y_min), int(x_max), int(y_max)]\n",
        "                })\n",
        "\n",
        "        return detected_objects\n",
        "\n",
        "    def detect_and_recognize_text(self, image_path):\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(\"Image not found at provided path.\")\n",
        "        bounding_boxes = detect_text(image, self.east_model_path)\n",
        "        recognized_texts = recognize_text(image, bounding_boxes)\n",
        "        pass\n",
        "\n",
        "    def generate_description(self, objects, texts):\n",
        "        # Create a text description based on detected objects and recognized text\n",
        "        object_descriptions = [f\"{obj['class_name']} ({obj['confidence']:.2f})\" for obj in objects]\n",
        "        objects_text = \", \".join(object_descriptions) if object_descriptions else \"No objects detected\"\n",
        "\n",
        "        texts_text = \", \".join(texts) if texts else \"No text detected\"\n",
        "\n",
        "        combined_description = f\"Detected objects: {objects_text}. Detected texts: {texts_text}\"\n",
        "\n",
        "        # Tokenize and generate description using GPT-2\n",
        "        inputs = self.tokenizer(combined_description, return_tensors=\"pt\")\n",
        "        outputs = self.gpt2_model.generate(inputs['input_ids'], max_length=525, num_return_sequences=1)\n",
        "\n",
        "        decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return decoded_output\n",
        "\n",
        "    def describe_image(self, image_path):\n",
        "        # Detect objects in the image\n",
        "        objects = self.detect_objects(image_path)\n",
        "\n",
        "        # Detect and recognize text in the image (Implement this part separately)\n",
        "\n",
        "        # Placeholder for recognized texts, replace with your implementation\n",
        "        texts = []\n",
        "\n",
        "        # Generate the image description\n",
        "        description = self.generate_description(objects, texts)\n",
        "        return description\n",
        "\n",
        "# Example usage\n",
        "east_model_path = '/content/frozen_east_text_detection.pb'\n",
        "yolo_model_path = 'yolov8n.pt'\n",
        "pipeline = ImageDescriptionPipeline(east_model_path, yolo_model_path)\n",
        "image_path = '124.jpg'\n",
        "description = pipeline.describe_image(image_path)\n",
        "print(description)\n",
        "\n",
        "   # def describe_image(self, image_path):\n",
        "  #      objects = self.detect_objects(image_path)\n",
        "   #     texts = self.detect_and_recognize_text(image_path)\n",
        "   #     description = self.generate_description(objects, texts)\n",
        "   #     return description\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "RpWu6URK0rTw",
        "outputId": "1a27c61f-7688-4da4-aa44-7f7dde71bba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 448x640 20 persons, 1 traffic light, 1 umbrella, 240.4ms\n",
            "Speed: 7.0ms preprocess, 240.4ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-3037fd444172>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDescriptionPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meast_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'124.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-3037fd444172>\u001b[0m in \u001b[0;36mdescribe_image\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescribe_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Detect objects in the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Detect and recognize text in the image (Implement this part separately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-3037fd444172>\u001b[0m in \u001b[0;36mdetect_objects\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Process detection results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdetected_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'pred'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "east_model_path = '/content/frozen_east_text_detection.pb'\n",
        "yolo_model_path = 'yolov8n.pt'\n",
        "pipeline = ImageDescriptionPipeline(east_model_path, yolo_model_path)\n",
        "image_path = '124.jpg'\n",
        "\n",
        "# Detect objects using YOLOv8\n",
        "detection_results = pipeline.detect_objects(image_path)\n",
        "\n",
        "# Generate description based on detection results\n",
        "description = pipeline.generate_description(detection_results)\n",
        "print(description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "HUJ5_lMNT0WU",
        "outputId": "5a34320f-0a6f-45d8-97bd-5c1fa5a54b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/124.jpg: 448x640 11 persons, 3 backpacks, 1105.1ms\n",
            "Speed: 13.8ms preprocess, 1105.1ms inference, 30.0ms postprocess per image at shape (1, 3, 448, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-b6e4a020a922>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Generate description based on detection results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-7560b1627e4f>\u001b[0m in \u001b[0;36mgenerate_description\u001b[0;34m(self, detection_results)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mdetected_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mdetected_objects_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetected_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Create a list of detected object descriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'names'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(results))\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuZFC5pLEGo6",
        "outputId": "a113483c-305c-4c59-fa79-52416bdcde12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "[ultralytics.engine.results.Results object with attributes:\n",
            "\n",
            "boxes: ultralytics.engine.results.Boxes object\n",
            "keypoints: None\n",
            "masks: None\n",
            "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
            "orig_img: array([[[102, 237, 239],\n",
            "        [ 93, 228, 230],\n",
            "        [ 80, 214, 218],\n",
            "        ...,\n",
            "        [ 45,  54,  57],\n",
            "        [ 46,  55,  59],\n",
            "        [ 48,  57,  61]],\n",
            "\n",
            "       [[ 95, 224, 227],\n",
            "        [ 85, 216, 219],\n",
            "        [ 74, 205, 208],\n",
            "        ...,\n",
            "        [ 49,  58,  61],\n",
            "        [ 50,  59,  63],\n",
            "        [ 51,  60,  64]],\n",
            "\n",
            "       [[ 91, 210, 212],\n",
            "        [ 83, 204, 206],\n",
            "        [ 76, 197, 199],\n",
            "        ...,\n",
            "        [ 54,  61,  64],\n",
            "        [ 54,  60,  65],\n",
            "        [ 55,  61,  66]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[109, 105, 104],\n",
            "        [101,  97,  96],\n",
            "        [112, 108, 107],\n",
            "        ...,\n",
            "        [111, 109, 108],\n",
            "        [108, 109, 107],\n",
            "        [116, 117, 115]],\n",
            "\n",
            "       [[108, 104, 103],\n",
            "        [112, 108, 107],\n",
            "        [105, 101, 100],\n",
            "        ...,\n",
            "        [120, 118, 117],\n",
            "        [112, 113, 111],\n",
            "        [121, 122, 120]],\n",
            "\n",
            "       [[112, 108, 107],\n",
            "        [117, 113, 112],\n",
            "        [112, 108, 107],\n",
            "        ...,\n",
            "        [128, 126, 125],\n",
            "        [120, 121, 119],\n",
            "        [129, 130, 128]]], dtype=uint8)\n",
            "orig_shape: (682, 1024)\n",
            "path: 'image0.jpg'\n",
            "probs: None\n",
            "save_dir: None\n",
            "speed: {'preprocess': 4.569530487060547, 'inference': 766.746997833252, 'postprocess': 2.248525619506836}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(self, image):\n",
        "    results = self.yolov8_model(image)\n",
        "    detected_objects = []\n",
        "    for result in results:\n",
        "        # Adapt this loop based on actual structure of result\n",
        "        for det in result:\n",
        "            # Validate and parse det based on its structure\n",
        "            if some_condition:  # Replace with actual condition\n",
        "                class_id = int(det[some_index])  # Replace some_index with actual index\n",
        "                class_name = self.yolov8_model.names[class_id]\n",
        "                confidence = float(det[another_index])  # Replace another_index with actual index\n",
        "                x_min, y_min, x_max, y_max = map(int, det[:4])  # Adjust slicing based on structure\n",
        "                detected_objects.append({\n",
        "                    'class_id': class_id,\n",
        "                    'class_name': class_name,\n",
        "                    'confidence': confidence,\n",
        "                    'bbox': [x_min, y_min, x_max, y_max]\n",
        "                })\n",
        "            else:\n",
        "                print(\"Unexpected detection format:\", det)\n",
        "    return detected_objects"
      ],
      "metadata": {
        "id": "YDP2LSMtENG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}